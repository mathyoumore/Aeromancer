{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8540b10f",
   "metadata": {},
   "source": [
    "### Edit this so that it's the month you want to generate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502c5c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To ZIP up all the reports from PythonAnywhere, use this command from a bash console\n",
    "# zip -r the_month_you_want.zip data/\n",
    "\n",
    "# Take all the CSV files you want to process (or, all the files for the month you want) and jam them in to /report\n",
    "# Change these two values to the month you want\n",
    "\n",
    "year = '2024'\n",
    "month = '01'\n",
    "\n",
    "# Do you only want the severe events?\n",
    "severe = True\n",
    "\n",
    "# Don't scroll away, but hit the double arrows above (or run every cell)\n",
    "# Check to make sure your monthly report was generated \n",
    "# Move all files from /report to /archive (if you want to keep things tidy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa111633",
   "metadata": {},
   "source": [
    "### Now hit the double right arrows in the menu above. The rest of this works on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87729108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from re import search, sub\n",
    "from os import listdir \n",
    "\n",
    "def fill_or_swap_duplicates(final_df, new_df, column='nws_id'):\n",
    "    if len(final_df) == 0:\n",
    "        # If the final df is empty, fill it with the file's contents\n",
    "        return new_df\n",
    "    else:\n",
    "        # Otherwise, remove all column values the final df has in common with the new file\n",
    "        # and insert the newer values (assumes the files are in order).\n",
    "        # This assumes the files are in alphabetical order, so careful\n",
    "        result_df = final_df[~final_df[column].isin(new_df[column])]\n",
    "        return pd.concat([result_df, new_df])\n",
    "\n",
    "\n",
    "directory = 'report/'\n",
    "salt = None\n",
    "old_date, old_type = -1, None\n",
    "final_df, final_events_df, final_locations_df = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "ugc_df = pd.read_csv('ugc_master.csv')\n",
    "county_zip_df = pd.read_csv(\n",
    "    'zip_code_database.csv',\n",
    "    dtype = {\n",
    "        'zip':str,\n",
    "        'state':str,\n",
    "        'ugc':str,\n",
    "        'clean_county':str,\n",
    "        'latitude':float,\n",
    "        'longitude':float,\n",
    "        'irs_estimated_population':int\n",
    "        })\n",
    "county_cleaner = (\n",
    "    r\"(North[ews]\\w*|South[ews]\\w*|Eastern|Western|Central|Interior|Coastal|\"\n",
    "    r\"\\s?Inland|\\s?County|\\s?Borough|Mountains?|Gorge|Area|Slopes|Upper|Lower)\\s?\")\n",
    "\n",
    "file_matcher = (\n",
    "    r\"(?P<year>20\\d{2})\"\n",
    "    r\"(?P<month>\\d{2})\"\n",
    "    r\"(?P<date>\\d{2})\"\n",
    "    r\"_(?P<hour>\\d{2}_)\"\n",
    "    r\"(?P<filetype>events|event_locations)\"\n",
    ")\n",
    "\n",
    "for file in sorted(listdir(directory)):\n",
    "    # Make sure it's a filename format we expect\n",
    "    # The result of this wille a dict with keys [year,month,date,hour,filetype]\n",
    "    try: \n",
    "        match_dict = search(file_matcher,file).groupdict()\n",
    "        if match_dict['year'] != year or match_dict['month'] != month:\n",
    "            print('Found a file outside of file format, skipping',file)\n",
    "            continue\n",
    "    except:\n",
    "        print('Failed to parse file name, skipping',file)\n",
    "        continue\n",
    "    \n",
    "    # ensure the files are sorted\n",
    "    # fill_or_swap_duplicates assumes the more recently viewed file is newer, \n",
    "    # and therefore has the most up-to-date information\n",
    "    # So this ends up being important \n",
    "    match_dict['date'] = int(match_dict['date'])\n",
    "    \n",
    "    if old_date < 0:\n",
    "        old_date = match_dict['date'] - 1\n",
    "        old_type = match_dict['filetype']\n",
    "    \n",
    "    if old_type != match_dict['filetype'] and old_date == match_dict['date']:\n",
    "        old_type = match_dict['filetype']\n",
    "    elif old_date > match_dict['date']:\n",
    "        assert False, \"Files are not sorted! Files must be sorted alphanumerically.\"\n",
    "    \n",
    "    old_date = match_dict['date']\n",
    "    \n",
    "    filepath = directory + file \n",
    "    if match_dict['filetype'] == 'events':\n",
    "        event_file_df = pd.read_csv(filepath, \n",
    "                              parse_dates = [2,3], \n",
    "                              date_parser=lambda col: pd.to_datetime(col, utc=True))\n",
    "        final_events_df = fill_or_swap_duplicates(final_events_df, event_file_df)\n",
    "    elif match_dict['filetype'] == 'event_locations':\n",
    "        locations_file_df = pd.read_csv(filepath)\n",
    "        final_locations_df = fill_or_swap_duplicates(final_locations_df, locations_file_df)\n",
    "    else:\n",
    "        print(\"I dunno what to do with this:\",filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023ab9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab0d087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are differences between the zip-county database and the NWS' county identification \n",
    "# This reconciles most of them, the ones that aren't caught tend to be geographic-specific locations \n",
    "ugc_df['clean_county'] = ugc_df['name'].apply(lambda x: sub(county_cleaner,'',x).strip())\n",
    "\n",
    "# Build up a ugc-zip data table based on the UGCs we know so far \n",
    "ugc_zip = ugc_df.merge(\n",
    "    county_zip_df, \n",
    "    left_on=['state','clean_county'], \n",
    "    right_on=['state','clean_county'],\n",
    "    how='left')\n",
    "\n",
    "# Rename, filter out to just the columns we need\n",
    "ugc_zip = ugc_zip.rename(columns={'ugc_x':'ugc'})[['ugc','name','state','clean_county','zip','irs_estimated_population']]\n",
    "\n",
    "# Make a table that joins our event_locations to this zip database\n",
    "loc_zip = final_locations_df.merge(\n",
    "    ugc_zip,\n",
    "    left_on='ugc',\n",
    "    right_on='ugc')\n",
    "\n",
    "# Pandas assumes nothing about datatypes, so let's convert some stuff\n",
    "loc_zip['nws_id'] = loc_zip['nws_id'].astype(str)\n",
    "loc_zip['zip'] = loc_zip['zip'].astype(str)\n",
    "final_events_df['nws_id'] = final_events_df['nws_id'].astype(str)\n",
    "\n",
    "#One more gigantic merge \n",
    "prep_final_df = final_events_df.merge(\n",
    "    loc_zip, \n",
    "    on='nws_id')\n",
    "\n",
    "# Filter out events we don't have a zip for (because they're probably mountains or a gorge or whatever)\n",
    "# Then filter out lesser events if Severe is True, otherwise bring in everyone \n",
    "final_df = ( prep_final_df[(~prep_final_df['zip'].isna()) &\n",
    "                           (True if not(severe) else prep_final_df['severity'].isin(['Severe','Extreme']))]\n",
    "           )\n",
    "\n",
    "# Final columns \n",
    "print_df = final_df[[\n",
    "    'clean_county',\n",
    "    'state',\n",
    "    'zip',\n",
    "    'start',\n",
    "    'end',\n",
    "    'severity',\n",
    "    'type',\n",
    "    'snow_min',\n",
    "    'snow_max',\n",
    "    'ice_min',\n",
    "    'ice_max',\n",
    "    'sleet_min',\n",
    "    'sleet_max',  \n",
    "]]\n",
    "\n",
    "print_df.reset_index(drop=True).to_csv(\"data/reports/{}{}_adverse_weather_report.csv\".format(year,month),index=False)\n",
    "print_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ced8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in print_df.groupby(['start_date','state']).count()['type'].reset_index().sort_values(['start_date', 'type'], ascending=[True,False]).itertuples():  \n",
    "    print(f\"{row.start_date} - {row.state} ({row.type})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "b3638c242192c72f73887d3947dd5669be9f44f0cbcb764454a7afd28782cb92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
