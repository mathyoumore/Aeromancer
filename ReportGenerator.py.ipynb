{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8540b10f",
   "metadata": {},
   "source": [
    "### Edit this so that it's the month you want to generate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "502c5c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2022\n",
    "month = 12\n",
    "\n",
    "# Do you only want the severe events?\n",
    "severe = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa111633",
   "metadata": {},
   "source": [
    "### Now hit the double right arrows in the menu above. The rest of this works on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87729108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from re import search, sub\n",
    "from os import listdir \n",
    "\n",
    "def fill_or_swap_duplicates(final_df, new_df, column='nws_id'):\n",
    "    if len(final_df) == 0:\n",
    "        # If the final df is empty, fill it with the file's contents\n",
    "        return new_df\n",
    "    else:\n",
    "        # Otherwise, remove all column values the final df has in common with the new file\n",
    "        # and insert the newer values (assumes the files are in order).\n",
    "        # This assumes the files are in alphabetical order, so careful\n",
    "        result_df = final_df[~final_df[column].isin(new_df[column])]\n",
    "        return pd.concat([result_df, new_df])\n",
    "\n",
    "\n",
    "directory = 'test/'\n",
    "salt = None\n",
    "old_date, old_type = -1, None\n",
    "final_df, final_events_df, final_locations_df = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "ugc_df = pd.read_csv('ugc_master.csv')\n",
    "county_zip_df = pd.read_csv(\n",
    "    'zip_code_database.csv',\n",
    "    dtype = {\n",
    "        'zip':str,\n",
    "        'state':str,\n",
    "        'ugc':str,\n",
    "        'clean_county':str,\n",
    "        'latitude':float,\n",
    "        'longitude':float,\n",
    "        'irs_estimated_population':int\n",
    "        })\n",
    "county_cleaner = (\n",
    "    r\"(North[ews]\\w*|South[ews]\\w*|Eastern|Western|Central|Interior|Coastal|\"\n",
    "    r\"\\s?Inland|\\s?County|\\s?Borough|Mountains?|Gorge|Area|Slopes|Upper|Lower)\\s?\")\n",
    "\n",
    "file_matcher = (\n",
    "    r\"_(?P<year>20\\d{2})\"\n",
    "    r\"(?P<month>\\d{2})\"\n",
    "    r\"(?P<date>\\d{2})\"\n",
    "    r\"_(?P<hour>\\d{2}_)\"\n",
    "    r\"(?P<filetype>events|event_locations)\"\n",
    ")\n",
    "\n",
    "for file in sorted(listdir(directory)):\n",
    "    # Make sure it's a filename format we expect\n",
    "    # The result of this wille a dict with keys [year,month,date,hour,filetype]\n",
    "    try: \n",
    "        match_dict = search(file_matcher,file).groupdict()\n",
    "        if int(match_dict['year']) != year or int(match_dict['month']) != month:\n",
    "            print('Found a file outside of file format, skipping',file)\n",
    "    except:\n",
    "        print('Failed to parse file name, skipping',file)\n",
    "        continue\n",
    "    \n",
    "    # ensure the files are sorted\n",
    "    # fill_or_swap_duplicates assumes the more recently viewed file is newer, \n",
    "    # and therefore has the most up-to-date information\n",
    "    # So this ends up being important \n",
    "    match_dict['date'] = int(match_dict['date'])\n",
    "    \n",
    "    if old_date < 0:\n",
    "        old_date = match_dict['date'] - 1\n",
    "        old_type = match_dict['filetype']\n",
    "    \n",
    "    if old_type != match_dict['filetype'] and old_date == match_dict['date']:\n",
    "        old_type = match_dict['filetype']\n",
    "    elif old_date > match_dict['date']:\n",
    "        assert False, \"Files are not sorted! Files must be sorted alphanumerically.\"\n",
    "    \n",
    "    old_date = match_dict['date']\n",
    "    \n",
    "    filepath = directory + file \n",
    "    if match_dict['filetype'] == 'events':\n",
    "        event_file_df = pd.read_csv(filepath, \n",
    "                              parse_dates = [2,3], \n",
    "                              date_parser=lambda col: pd.to_datetime(col, utc=True))\n",
    "        final_events_df = fill_or_swap_duplicates(final_events_df, event_file_df)\n",
    "    elif match_dict['filetype'] == 'event_locations':\n",
    "        locations_file_df = pd.read_csv(filepath)\n",
    "        final_locations_df = fill_or_swap_duplicates(final_locations_df, locations_file_df)\n",
    "    else:\n",
    "        print(\"I dunno what to do with this:\",filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ab0d087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_county</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>severity</th>\n",
       "      <th>type</th>\n",
       "      <th>snow_min</th>\n",
       "      <th>snow_max</th>\n",
       "      <th>ice_min</th>\n",
       "      <th>ice_max</th>\n",
       "      <th>sleet_min</th>\n",
       "      <th>sleet_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109908</th>\n",
       "      <td>Big Island Summit</td>\n",
       "      <td>HI</td>\n",
       "      <td>nan</td>\n",
       "      <td>2022-12-19 13:31:00+00:00</td>\n",
       "      <td>2022-12-20 16:00:00+00:00</td>\n",
       "      <td>Severe</td>\n",
       "      <td>Winter Storm Warning</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109909</th>\n",
       "      <td>Big Island Summit</td>\n",
       "      <td>HI</td>\n",
       "      <td>nan</td>\n",
       "      <td>2022-12-19 13:31:00+00:00</td>\n",
       "      <td>2022-12-20 16:00:00+00:00</td>\n",
       "      <td>Severe</td>\n",
       "      <td>Winter Storm Warning</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109910</th>\n",
       "      <td>Big Island Summit</td>\n",
       "      <td>HI</td>\n",
       "      <td>nan</td>\n",
       "      <td>2022-12-19 13:31:00+00:00</td>\n",
       "      <td>2022-12-20 16:00:00+00:00</td>\n",
       "      <td>Severe</td>\n",
       "      <td>Winter Storm Warning</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109911</th>\n",
       "      <td>Big Island Summit</td>\n",
       "      <td>HI</td>\n",
       "      <td>nan</td>\n",
       "      <td>2022-12-19 13:31:00+00:00</td>\n",
       "      <td>2022-12-20 16:00:00+00:00</td>\n",
       "      <td>Severe</td>\n",
       "      <td>Winter Storm Warning</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109912</th>\n",
       "      <td>Big Island Summit</td>\n",
       "      <td>HI</td>\n",
       "      <td>nan</td>\n",
       "      <td>2022-12-19 13:31:00+00:00</td>\n",
       "      <td>2022-12-20 16:00:00+00:00</td>\n",
       "      <td>Severe</td>\n",
       "      <td>Winter Storm Warning</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484011</th>\n",
       "      <td>West Glacier Region</td>\n",
       "      <td>MT</td>\n",
       "      <td>nan</td>\n",
       "      <td>2022-12-19 19:52:00+00:00</td>\n",
       "      <td>2022-12-22 00:00:00+00:00</td>\n",
       "      <td>Severe</td>\n",
       "      <td>Winter Storm Warning</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484012</th>\n",
       "      <td>Bitterroot/Sapphire</td>\n",
       "      <td>MT</td>\n",
       "      <td>nan</td>\n",
       "      <td>2022-12-19 19:52:00+00:00</td>\n",
       "      <td>2022-12-22 00:00:00+00:00</td>\n",
       "      <td>Severe</td>\n",
       "      <td>Winter Storm Warning</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484013</th>\n",
       "      <td>Kootenai/Cabinet Region</td>\n",
       "      <td>MT</td>\n",
       "      <td>nan</td>\n",
       "      <td>2022-12-19 19:52:00+00:00</td>\n",
       "      <td>2022-12-22 00:00:00+00:00</td>\n",
       "      <td>Severe</td>\n",
       "      <td>Winter Storm Warning</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484014</th>\n",
       "      <td>Butte/Blackfoot Region</td>\n",
       "      <td>MT</td>\n",
       "      <td>nan</td>\n",
       "      <td>2022-12-19 19:52:00+00:00</td>\n",
       "      <td>2022-12-22 00:00:00+00:00</td>\n",
       "      <td>Severe</td>\n",
       "      <td>Winter Storm Warning</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484015</th>\n",
       "      <td>Missoula/Bitterroot Valleys</td>\n",
       "      <td>MT</td>\n",
       "      <td>nan</td>\n",
       "      <td>2022-12-19 19:52:00+00:00</td>\n",
       "      <td>2022-12-22 00:00:00+00:00</td>\n",
       "      <td>Severe</td>\n",
       "      <td>Winter Storm Warning</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>282525 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       clean_county state  zip                     start  \\\n",
       "109908            Big Island Summit    HI  nan 2022-12-19 13:31:00+00:00   \n",
       "109909            Big Island Summit    HI  nan 2022-12-19 13:31:00+00:00   \n",
       "109910            Big Island Summit    HI  nan 2022-12-19 13:31:00+00:00   \n",
       "109911            Big Island Summit    HI  nan 2022-12-19 13:31:00+00:00   \n",
       "109912            Big Island Summit    HI  nan 2022-12-19 13:31:00+00:00   \n",
       "...                             ...   ...  ...                       ...   \n",
       "484011          West Glacier Region    MT  nan 2022-12-19 19:52:00+00:00   \n",
       "484012          Bitterroot/Sapphire    MT  nan 2022-12-19 19:52:00+00:00   \n",
       "484013      Kootenai/Cabinet Region    MT  nan 2022-12-19 19:52:00+00:00   \n",
       "484014       Butte/Blackfoot Region    MT  nan 2022-12-19 19:52:00+00:00   \n",
       "484015  Missoula/Bitterroot Valleys    MT  nan 2022-12-19 19:52:00+00:00   \n",
       "\n",
       "                             end severity                  type  snow_min  \\\n",
       "109908 2022-12-20 16:00:00+00:00   Severe  Winter Storm Warning       0.0   \n",
       "109909 2022-12-20 16:00:00+00:00   Severe  Winter Storm Warning       0.0   \n",
       "109910 2022-12-20 16:00:00+00:00   Severe  Winter Storm Warning       0.0   \n",
       "109911 2022-12-20 16:00:00+00:00   Severe  Winter Storm Warning       0.0   \n",
       "109912 2022-12-20 16:00:00+00:00   Severe  Winter Storm Warning       0.0   \n",
       "...                          ...      ...                   ...       ...   \n",
       "484011 2022-12-22 00:00:00+00:00   Severe  Winter Storm Warning       6.0   \n",
       "484012 2022-12-22 00:00:00+00:00   Severe  Winter Storm Warning       6.0   \n",
       "484013 2022-12-22 00:00:00+00:00   Severe  Winter Storm Warning       5.0   \n",
       "484014 2022-12-22 00:00:00+00:00   Severe  Winter Storm Warning       4.0   \n",
       "484015 2022-12-22 00:00:00+00:00   Severe  Winter Storm Warning       7.0   \n",
       "\n",
       "        snow_max  ice_min  ice_max  sleet_min  sleet_max  \n",
       "109908       8.0      NaN      NaN        NaN        NaN  \n",
       "109909       8.0      NaN      NaN        NaN        NaN  \n",
       "109910       8.0      NaN      NaN        NaN        NaN  \n",
       "109911       8.0      NaN      NaN        NaN        NaN  \n",
       "109912       8.0      NaN      NaN        NaN        NaN  \n",
       "...          ...      ...      ...        ...        ...  \n",
       "484011      11.0      NaN      NaN        NaN        NaN  \n",
       "484012      11.0      NaN      NaN        NaN        NaN  \n",
       "484013      11.0      NaN      NaN        NaN        NaN  \n",
       "484014       6.0      NaN      NaN        NaN        NaN  \n",
       "484015      10.0      NaN      NaN        NaN        NaN  \n",
       "\n",
       "[282525 rows x 13 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are differences between the zip-county database and the NWS' county identification \n",
    "# This reconciles most of them, the ones that aren't caught tend to be geographic-specific locations \n",
    "ugc_df['clean_county'] = ugc_df['name'].apply(lambda x: sub(county_cleaner,'',x).strip())\n",
    "\n",
    "# Build up a ugc-zip data table based on the UGCs we know so far \n",
    "ugc_zip = ugc_df.merge(\n",
    "    county_zip_df, \n",
    "    left_on=['state','clean_county'], \n",
    "    right_on=['state','clean_county'],\n",
    "    how='left')\n",
    "\n",
    "# Rename, filter out to just the columns we need\n",
    "ugc_zip = ugc_zip.rename(columns={'ugc_x':'ugc'})[['ugc','name','state','clean_county','zip','irs_estimated_population']]\n",
    "\n",
    "# Make a table that joins our event_locations to this zip database\n",
    "loc_zip = final_locations_df.merge(\n",
    "    ugc_zip,\n",
    "    left_on='ugc',\n",
    "    right_on='ugc')\n",
    "\n",
    "# Pandas assumes nothing about datatypes, so let's convert some stuff\n",
    "loc_zip['nws_id'] = loc_zip['nws_id'].astype(str)\n",
    "loc_zip['zip'] = loc_zip['zip'].astype(str)\n",
    "final_events_df['nws_id'] = final_events_df['nws_id'].astype(str)\n",
    "\n",
    "#One more gigantic merge \n",
    "prep_final_df = final_events_df.merge(\n",
    "    loc_zip, \n",
    "    on='nws_id')\n",
    "\n",
    "# Filter out events we don't have a zip for (because they're probably mountains or a gorge or whatever)\n",
    "# Then filter out lesser events if Severe is True, otherwise bring in everyone \n",
    "final_df = ( prep_final_df[(~prep_final_df['zip'].isna()) &\n",
    "                           (True if not(severe) else prep_final_df['severity'].isin(['Severe','Extreme']))]\n",
    "           )\n",
    "\n",
    "# Final columns \n",
    "print_df = final_df[[\n",
    "    'clean_county',\n",
    "    'state',\n",
    "    'zip',\n",
    "    'start',\n",
    "    'end',\n",
    "    'severity',\n",
    "    'type',\n",
    "    'snow_min',\n",
    "    'snow_max',\n",
    "    'ice_min',\n",
    "    'ice_max',\n",
    "    'sleet_min',\n",
    "    'sleet_max',  \n",
    "]]\n",
    "\n",
    "print_df.reset_index(drop=True).to_csv(\"data/reports/{}{}_adverse_weather_report.csv\".format(year,month),index=False)\n",
    "print_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "b3638c242192c72f73887d3947dd5669be9f44f0cbcb764454a7afd28782cb92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
